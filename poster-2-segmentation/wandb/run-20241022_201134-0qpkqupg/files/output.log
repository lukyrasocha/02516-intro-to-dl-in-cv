â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m1[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.7291[0m, Validation Loss: [1;36m0.6481[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m2[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.6505[0m, Validation Loss: [1;36m0.6503[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m3[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.5693[0m, Validation Loss: [1;36m2.7541[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m4[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.9556[0m, Validation Loss: [1;36m0.6193[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m5[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.5840[0m, Validation Loss: [1;36m0.5781[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m6[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.5176[0m, Validation Loss: [1;36m0.4755[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m7[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.5293[0m, Validation Loss: [1;36m0.4579[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m8[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.4967[0m, Validation Loss: [1;36m0.4547[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m9[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.5026[0m, Validation Loss: [1;36m3.4580[0m
â„¹ [1;34mINFO[0m: Epoch [1m[[0m[1;36m10[0m/[1;36m20[0m[1m][0m, Training Loss: [1;36m0.9455[0m, Validation Loss: [1;36m0.5506[0m
Traceback (most recent call last):
  File "/zhome/20/1/209339/02516-intro-to-dl-in-cv/poster-2-segmentation/main.py", line 95, in <module>
    train_model(encdec_ph2_model, ph2_train_loader, ph2_val_loader, loss_fn, optimizer,wandb_config=config, num_epochs=MAX_EPOCHS, device=DEVICE)
  File "/zhome/20/1/209339/02516-intro-to-dl-in-cv/poster-2-segmentation/models/train.py", line 18, in train_model
    for images, masks in train_loader:
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py", line 757, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/zhome/20/1/209339/02516-intro-to-dl-in-cv/poster-2-segmentation/utils/load_data.py", line 56, in __getitem__
    mask = self.transform(mask)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torchvision/transforms/transforms.py", line 95, in __call__
    img = t(img)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torchvision/transforms/transforms.py", line 354, in forward
    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torchvision/transforms/functional.py", line 477, in resize
    return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/torchvision/transforms/_functional_pil.py", line 250, in resize
    return img.resize(tuple(size[::-1]), interpolation)
  File "/zhome/20/1/209339/venv/project2_venv/lib64/python3.9/site-packages/PIL/Image.py", line 2365, in resize
    return self._new(self.im.resize(size, resample, box))
KeyboardInterrupt
